{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Configuraci√≥n y Conexi√≥n\n",
    "# ==========================================\n",
    "print(\"üîå Estableciendo conexi√≥n con la base de datos...\")\n",
    "load_dotenv() # Carga las variables del archivo .env\n",
    "\n",
    "# Verificamos credenciales\n",
    "required_vars = ['DB_USER', 'DB_PASSWORD', 'DB_HOST', 'DB_PORT', 'DB_NAME']\n",
    "if not all(os.getenv(var) for var in required_vars):\n",
    "    raise ValueError(\"‚ùå Faltan variables en el archivo .env\")\n",
    "\n",
    "# Crear String de Conexi√≥n\n",
    "DB_URL = f\"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# 2. Carga del CSV Limpio\n",
    "# ==========================================\n",
    "csv_file = 'ventas_limpio_auto.csv'\n",
    "print(f\"üìÇ Leyendo archivo: {csv_file}...\")\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # üîß REPARACI√ìN: Manejo robusto de fechas\n",
    "    print(\"   üîß Procesando columna de fecha...\")\n",
    "    \n",
    "    # Primero, reemplazar \"Nan\" strings por valores NaN reales\n",
    "    df['fecha'] = df['fecha'].replace('Nan', np.nan)\n",
    "    \n",
    "    # Luego convertir a datetime manejando errores\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # Verificar si hay fechas inv√°lidas\n",
    "    fechas_invalidas = df['fecha'].isna().sum()\n",
    "    if fechas_invalidas > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Advertencia: {fechas_invalidas} registros con fechas inv√°lidas ser√°n eliminados\")\n",
    "        # Eliminar registros con fechas inv√°lidas\n",
    "        df = df.dropna(subset=['fecha'])\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {df.shape[0]} registros v√°lidos\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå No se encuentra 'ventas_limpio_auto.csv'. Ejecuta la limpieza primero.\")\n",
    "\n",
    "# 3. Normalizaci√≥n (Creaci√≥n de Dimensiones)\n",
    "# ==========================================\n",
    "print(\"\\nüîÑ Iniciando Normalizaci√≥n (Modelo Estrella)...\")\n",
    "\n",
    "# --- A. Dimensi√≥n Producto ---\n",
    "# Extraemos √∫nicos de producto y tipo\n",
    "dim_producto = df[['producto', 'tipo_producto']].drop_duplicates().reset_index(drop=True)\n",
    "dim_producto['id_producto'] = dim_producto.index + 1 # Crear Primary Key\n",
    "# Mapeamos el ID al dataframe principal\n",
    "df = df.merge(dim_producto, on=['producto', 'tipo_producto'], how='left')\n",
    "\n",
    "# --- B. Dimensi√≥n Geograf√≠a ---\n",
    "# Extraemos √∫nicos de ciudad y pais\n",
    "dim_geografia = df[['ciudad', 'pais']].drop_duplicates().reset_index(drop=True)\n",
    "dim_geografia['id_geografia'] = dim_geografia.index + 1\n",
    "df = df.merge(dim_geografia, on=['ciudad', 'pais'], how='left')\n",
    "\n",
    "# --- C. Dimensi√≥n Canal/Cliente ---\n",
    "# Extraemos √∫nicos de tipo de venta y cliente\n",
    "dim_canal = df[['tipo_venta', 'tipo_cliente']].drop_duplicates().reset_index(drop=True)\n",
    "dim_canal['id_canal'] = dim_canal.index + 1\n",
    "df = df.merge(dim_canal, on=['tipo_venta', 'tipo_cliente'], how='left')\n",
    "\n",
    "# --- D. Tabla de Hechos (Fact Table) ---\n",
    "# Seleccionamos solo las m√©tricas y los IDs (Foreign Keys)\n",
    "fact_ventas = df[[\n",
    "    'fecha', \n",
    "    'id_producto', \n",
    "    'id_geografia', \n",
    "    'id_canal', \n",
    "    'cantidad', \n",
    "    'precio_unitario', \n",
    "    'descuento', \n",
    "    'costo_envio', \n",
    "    'total_ventas'\n",
    "]].copy()\n",
    "\n",
    "print(\"‚úÖ Datos normalizados en memoria.\")\n",
    "\n",
    "# 4. Carga a PostgreSQL (Load)\n",
    "# ==========================================\n",
    "print(\"\\nüöÄ Subiendo datos a PostgreSQL (esto puede tardar unos minutos)...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Opcional: Limpiar tablas anteriores si existen (Orden inverso a la creaci√≥n por las FK)\n",
    "        print(\"   üßπ Limpiando tablas existentes...\")\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS fact_ventas CASCADE;\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS dim_producto CASCADE;\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS dim_geografia CASCADE;\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS dim_canal CASCADE;\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # Subir Dimensiones\n",
    "    print(\"   ‚¨ÜÔ∏è Subiendo Dimensi√≥n: Productos...\")\n",
    "    dim_producto.to_sql('dim_producto', engine, index=False, if_exists='replace')\n",
    "    # A√±adir PK a la tabla creada\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"ALTER TABLE dim_producto ADD PRIMARY KEY (id_producto);\"))\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"   ‚¨ÜÔ∏è Subiendo Dimensi√≥n: Geograf√≠a...\")\n",
    "    dim_geografia.to_sql('dim_geografia', engine, index=False, if_exists='replace')\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"ALTER TABLE dim_geografia ADD PRIMARY KEY (id_geografia);\"))\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"   ‚¨ÜÔ∏è Subiendo Dimensi√≥n: Canal...\")\n",
    "    dim_canal.to_sql('dim_canal', engine, index=False, if_exists='replace')\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"ALTER TABLE dim_canal ADD PRIMARY KEY (id_canal);\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # Subir Hechos (Usamos chunksize para no saturar la memoria en 1.25M de filas)\n",
    "    print(\"   ‚¨ÜÔ∏è Subiendo Tabla de Hechos: Ventas (por lotes)...\")\n",
    "    fact_ventas.to_sql('fact_ventas', engine, index=False, if_exists='replace', chunksize=10000)\n",
    "    \n",
    "    # A√±adir Foreign Keys (Integridad Referencial)\n",
    "    print(\"   üîó Estableciendo relaciones (Foreign Keys)...\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            ALTER TABLE fact_ventas \n",
    "            ADD CONSTRAINT fk_producto FOREIGN KEY (id_producto) REFERENCES dim_producto(id_producto),\n",
    "            ADD CONSTRAINT fk_geografia FOREIGN KEY (id_geografia) REFERENCES dim_geografia(id_geografia),\n",
    "            ADD CONSTRAINT fk_canal FOREIGN KEY (id_canal) REFERENCES dim_canal(id_canal);\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"\\nüéâ ¬°PROCESO EXITOSO!\")\n",
    "    print(\"   Las tablas han sido creadas y pobladas en PostgreSQL.\")\n",
    "    print(f\"   - dim_producto: {dim_producto.shape[0]} productos\")\n",
    "    print(f\"   - dim_geografia: {dim_geografia.shape[0]} ubicaciones\")\n",
    "    print(f\"   - dim_canal: {dim_canal.shape[0]} canales\")\n",
    "    print(f\"   - fact_ventas: {fact_ventas.shape[0]} transacciones\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ocurri√≥ un error durante la carga: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
